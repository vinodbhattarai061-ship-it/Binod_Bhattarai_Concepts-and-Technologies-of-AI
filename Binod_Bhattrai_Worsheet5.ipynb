{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS_a9WiOBJ09",
        "outputId": "8e45f0cf-6b22-4ac6-b53d-eae23350cb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "     Math  Reading  Writing\n",
            "0      48       68       63\n",
            "1      62       81       72\n",
            "2      79       80       78\n",
            "3      76       83       79\n",
            "4      59       64       62\n",
            "..    ...      ...      ...\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "[1000 rows x 3 columns]\n",
            "\n",
            "Top 5 Rows:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 Rows:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "\n",
            "Dataset Description:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ],
      "source": [
        "# Task 1\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"student.csv\")\n",
        "\n",
        "# 1. Observe dataset\n",
        "print(\"Dataset Preview:\")\n",
        "print(data)\n",
        "\n",
        "# 2. Top 5 rows\n",
        "print(\"\\nTop 5 Rows:\")\n",
        "print(data.head())\n",
        "\n",
        "# 3. Bottom 5 rows\n",
        "print(\"\\nBottom 5 Rows:\")\n",
        "print(data.tail())\n",
        "\n",
        "# 4. Dataset Information\n",
        "print(\"\\nDataset Info:\")\n",
        "data.info()\n",
        "\n",
        "# 5. Descriptive Statistics\n",
        "print(\"\\nDataset Description:\")\n",
        "print(data.describe())\n",
        "\n",
        "# 6. Split Features and Target\n",
        "X = data[['Math', 'Reading']].values\n",
        "Y = data['Writing'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"student.csv\")\n",
        "\n",
        "# Select features and target\n",
        "X = df[[\"Math\", \"Reading\"]]\n",
        "Y = df[\"Writing\"]\n",
        "\n",
        "# No bias / intercept term assumed\n",
        "\n",
        "# Feature matrix X (d x n)\n",
        "X_matrix = X.values.T\n",
        "\n",
        "# Target vector Y (n x 1)\n",
        "Y_matrix = Y.values\n",
        "\n",
        "# Weight vector W (d x 1)\n",
        "W = np.random.rand(X_matrix.shape[0])\n",
        "\n",
        "# Linear model: Y = WᵀX\n",
        "Y_pred = np.dot(W.T, X_matrix)\n",
        "\n",
        "print(\"\\nFeature Matrix X:\\n\", X_matrix)\n",
        "print(\"\\nWeight Vector W:\\n\", W)\n",
        "print(\"\\nTarget Vector Y:\\n\", Y_matrix)\n",
        "print(\"\\nPredicted Y (WᵀX):\\n\", Y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFT61uVTPT5w",
        "outputId": "2606b247-6ece-439a-8865-ee54204d6ec8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Matrix X:\n",
            " [[48 62 79 ... 89 83 66]\n",
            " [68 81 80 ... 87 82 66]]\n",
            "\n",
            "Weight Vector W:\n",
            " [0.3924563  0.04578883]\n",
            "\n",
            "Target Vector Y:\n",
            " [ 63  72  78  79  62  85  83  41  80  77  64  90  45  77  70  46  76  44\n",
            "  85  72  53  66  75  49  84  83  68  66  77  78  74  83  72  65  46  66\n",
            "  50  79  68  46  86  70  61  53  72  75  50  77 100  81 100  87  78  48\n",
            "  50  44  48  43  67  78  58  91  92  78  42  85  73  83  61  58  60  55\n",
            "  48  62  68  59  62  48  74  63  80  79  73  79  45  67  89  77  81  88\n",
            "  53  68  79  77  63  73  60  67 100  79  26  51  80  57  41  78  68  49\n",
            "  76  41  71  77  89  86  55  80  56  74  85  80  73  74  86  56  53  44\n",
            "  41  59  71  81  74  78  67  53  56  75  82  79  99  76  59  96  75  61\n",
            "  56  88  65 100  79  55  61  83  74  59  54  47  82  74  59  74  84  59\n",
            "  43  65  61  78  84  73  73  92  63  72  61  59  70  87  78  65  73  62\n",
            "  69  55  73  63  67  86  78  85  83  80  60  90  56  70  55  80  82  60\n",
            "  78  76  94  75  68  71  85  46  58  46  84  58  57  59  77  63  68  99\n",
            "  48  91  57  80  46  75  59  87  82  79  66  68  66  61  66  63  72  73\n",
            "  77  84  83  42  72  76  76  39  74  43  63  74  52  31  65  45  87  63\n",
            "  51  82  86  76  27  70  79  66  61  62  47  17  65  76  75  66  59  61\n",
            "  93  40  66  43  71  64  55  86  65  70  65  53  49  67  76  95  76  48\n",
            "  60  53  69  78  62  66  51  52  46  42  77  57 100  84  68  48  72  50\n",
            "  72  55  72  77  56  94  67  82  75  80  60  73  74  62  53  69  75  60\n",
            "  58  71  87  74  87  73  78  76  74  55  94  71  76  59  91  57  83  59\n",
            "  93  64  58  79  96  76  64  70  80  33  95  64  92  34  72  81  57  79\n",
            "  84  82  54  45  54  62  49  74  59  63  83  62  72  72  65  65  54  78\n",
            "  82  85  74  83  71  83  77  66  75  52  68  84  67  70  41  91  46  58\n",
            "  67  70  83  64 100  49  77  57  67  80  74  41  67  59  86  88  57  80\n",
            "  58  52  31  84  97  71  62  58  71  41  66 100  51  35  81  94  72  38\n",
            "  82  79  55  75  90  95  65  39  85  86  54  93  69  84  78  58  73  60\n",
            "  44  67  69  55  59  88  42  78  84  68  66  51  43  38  69  90  73  67\n",
            "  57  81  63  80  78  65  74  80  60  60  63  64  72  51  71  63  82  76\n",
            "  39  79  48  70  90  73  58 100  80  75  72  79  52  56  65  45  59  61\n",
            "  47  62  83  90  76  72  69  57  56  40  79  48  57  47  78  45  74  69\n",
            "  59  85  45  54  72  74  75  55  49  53  83  22 100  67  83  46  43  74\n",
            "  64  35  67  87  77  91  74  96  82  78  73  52  91  66  67  71  74  71\n",
            "  61  47  76  85  93  41  81  86  53  91  68  96  48  71  75  72  71  62\n",
            "  67  53  74  63  82  57  69  52  91  73  73  75  36  71  62 100  50  74\n",
            "  60  75  83  83 100  67  71  77  67  95  52  71  74  60  67  79  75  95\n",
            "  69  80  48  61  82  39  70  70  69  32  79  53  59  83 100  80  80  82\n",
            "  56  83  85  88  81  95  63  70  89  59  56  62  95  63  82  69  58  74\n",
            "  66  82  94  70  78  63  91  70  62  79  65  74  56  65 100  70  66  54\n",
            "  72  90  56  65  50  95  38  76  84  76  55  85  70  73  80  83  53  67\n",
            " 100  67  44  96  48  77 100  40  91  55  41  25  63  59  63  77  46  49\n",
            "  46  93  39  58  87  57  77 100  65  34  87  81  63  69  74  70  93  63\n",
            "  81  81  63  87  76  54  89  63  76  79  75  50  36  82  83  85  82  41\n",
            "  82  45  57  88  81  98  61  95  84  71  52  71  90  75  62  63  86  70\n",
            "  77  68  80  67  67  89  60  79  80  78  70  72  43  14  54  92  71  65\n",
            "  58  56  67  64  81  55  45  86  52  75  81  62  42  21  72  55  66  69\n",
            "  86  67  78  85  66  47 100  63  62  61  69  57  76  52  47  51  61  45\n",
            "  59  81  65  53  61  90  74  62  67  50  84  70  52  92  65  65  67  72\n",
            "  66  62  99  62  53  57  78  56  87  79  63  87  86  75  70  60  49  41\n",
            "  78  58  75  89  34  60  80  85  73  58  69  74  52  58  79  86  61  68\n",
            "  67  48  65  73  57  73  57  80  85  81  61  69 100  99  92  72  57  44\n",
            "  59  62  93  64  57  72  40  85  60  83  63  74  44  61  74  68  78  50\n",
            "  70  68  82  46  96 100  44  41  95  79  67  52  87  75  61  42  60  57\n",
            "  64  52  68  58  93  75  77  66  63  90  43  65  95  86  31  95  52  63\n",
            "  87  70  59  84  79  77  75  66  69  85  63  50  58  80  47  55  61  87\n",
            "  77  54  66  68  54  69  74  81  72  61  76  63  64  73  62  92  69  70\n",
            "  65  53  74  61  80  85  62  80  83  56  76  52  51  74  57  63  61  87\n",
            "  60  54  89  67  56  70  90  94  78  72]\n",
            "\n",
            "Predicted Y (WᵀX):\n",
            " [21.95154293 28.04118592 34.6671541  33.62715172 26.08540684 30.92574649\n",
            " 31.31820278 20.25085365 27.51136312 37.3227705  27.4917199  32.28627071\n",
            " 23.99226126 35.36048902 27.14505244 21.24506719 26.79838498 19.85839735\n",
            " 31.98539209 32.86838475 28.5578679  25.39207192 29.59136788 27.105766\n",
            " 30.62486786 30.8080232  27.3543534  25.41171514 35.75294532 28.87188734\n",
            " 31.39013723 31.2004795  28.50557666 32.98610803 25.81717623 21.23856479\n",
            " 22.049623   32.95996242 30.46785814 18.49787313 32.63293817 34.1373313\n",
            " 28.34856694 22.25892396 27.05347477 30.37628047 22.4420793  28.50557666\n",
            " 39.89995004 26.15083889 43.82451299 32.72451584 28.04118592 19.99576386\n",
            " 20.73488761 23.04383655 26.85067621 21.5198002  33.37856433 31.18083628\n",
            " 26.06576362 36.393989   43.73293532 32.05732654 26.3012102  38.35627048\n",
            " 25.09119329 41.47627762 29.43435816 28.5578679  29.48014699 25.85646267\n",
            " 22.60559142 26.4320743  26.52365197 20.47979782 27.40014223 13.92576409\n",
            " 36.05382394 30.05575863 35.93610066 28.82609851 25.66680493 31.39013723\n",
            " 16.62716932 30.00996979 32.24048188 32.26662749 31.08925861 33.18890659\n",
            " 23.52787051 31.2069819  26.84417381 34.99417834 25.78452821 24.39785837\n",
            " 22.06926622 28.00189948 37.54521227 36.99574625 14.37051162 22.4420793\n",
            " 30.16697951 15.12277619 18.66138525 32.26662749 35.61557881 20.4601546\n",
            " 32.61329495 16.3262907  25.96768355 30.44171252 40.04381895 33.6010061\n",
            " 19.07348476 30.53329019 21.79453321 30.55943581 39.65136265 36.00803511\n",
            " 28.71487762 34.6671541  32.88802796 34.76523417 18.33436101 21.03576624\n",
            " 13.95190971 23.36435839 37.21154961 40.69136503 33.69908617 26.07890444\n",
            " 28.23084366 24.14927098 26.45821992 34.2746978  40.50820969 38.19926076\n",
            " 34.34012986 39.16732869 24.60715933 37.10696714 29.93803534 29.06154508\n",
            " 25.04540446 29.40171014 35.98839189 39.89995004 34.92224389 21.28435363\n",
            " 29.0876907  34.89609827 29.93803534 26.94225388 27.12540922 19.46594106\n",
            " 33.16276097 29.68294555 21.02926384 31.2069819  32.44978283 22.38978806\n",
            " 18.31471779 25.11083651 31.85452798 32.77680708 41.61364413 29.29048925\n",
            " 25.18277096 43.59556882 23.5017249  30.16697951 31.18733868 24.33242632\n",
            " 26.54329519 34.03925123 36.85837975 29.52593583 27.51136312 23.7764579\n",
            " 31.67137264 26.59558642 31.62558381 27.60944319 23.22048949 31.27241395\n",
            " 30.65101348 27.92346263 38.24504959 39.21311752 26.20313013 43.32083581\n",
            " 29.52593583 36.47242585 18.60909401 30.90610327 39.69715149 29.80066884\n",
            " 28.20469804 35.22312252 38.95802773 29.15312275 26.08540684 34.46435554\n",
            " 38.08153747 14.01734176 27.42628785 19.16506243 32.03768332 22.25242156\n",
            " 28.11962277 30.9583945  36.00803511 20.61716432 28.57751112 43.59556882\n",
            " 17.80453821 32.95346002 25.60137288 37.9899598  26.36664225 31.25277073\n",
            " 31.14154984 35.36048902 30.09504506 30.69680231 23.91382441 30.8145256\n",
            " 24.95382679 36.35470257 19.7864629  27.56365435 35.66136765 27.41978545\n",
            " 30.39592369 33.4636396  34.36627547 14.52752134 27.58329757 36.62943558\n",
            " 28.94382179 17.60173965 25.66680493 23.39050401 25.73873938 30.44171252\n",
            " 22.99804771 13.2847204  27.60944319 16.14313536 37.66293556 28.74102324\n",
            " 20.08734153 30.97153532 31.84802558 31.13504744 14.04348738 27.41978545\n",
            " 32.03768332 28.78681207 27.70102086 28.04768832 26.9683995   8.32665747\n",
            " 28.04768832 41.52206646 26.8180282  26.40592868 32.73101824 22.89996764\n",
            " 43.64135765 19.85839735 29.36242371 15.35822277 35.84452299 27.07962039\n",
            " 25.85646267 36.62943558 34.46435554 28.62329995 31.11540423 16.30014508\n",
            " 20.38822015 31.18733868 30.90610327 35.38013224 31.64522703 25.95454274\n",
            " 17.96154793 23.73066907 34.41856671 32.17504982 29.70909117 31.80873915\n",
            " 24.8426059  24.45014961 20.34243132 20.76103323 34.18312014 27.03383155\n",
            " 39.23276074 39.09539423 28.39435578 21.33664486 28.62329995 30.4285717\n",
            " 30.53979259 20.89839973 33.09732892 36.46592345 22.62523464 34.43170753\n",
            " 22.80838997 34.94188711 32.17504982 31.69101586 30.07540184 32.61329495\n",
            " 25.92189472 30.44821492 22.88032443 30.40242609 31.06961539 34.16347692\n",
            " 30.79488238 32.12926099 38.68329472 24.53522488 38.80101801 38.15347192\n",
            " 32.10311537 30.2847028  26.91610827 20.61716432 42.85644506 21.5132978\n",
            " 31.11540423 30.26505958 36.7864453  22.5794458  37.02189187 24.72488262\n",
            " 42.67328973 24.05119091 21.23856479 37.27698166 40.93995242 27.58329757\n",
            " 28.83260091 34.5297876  25.64065931 17.51016198 35.44556429 30.35663725\n",
            " 39.0496054  14.50787812 33.9999648  34.11118568 25.50979521 29.51943343\n",
            " 32.14890421 40.6455762  25.2350622  23.46243846 28.1654116  31.04997217\n",
            " 20.25085365 33.21505221 29.36242371 31.02382656 38.5197826  31.27891635\n",
            " 29.06154508 34.5297876  31.85452798 22.06926622 25.73873938 31.55364936\n",
            " 34.41206431 41.12961016 38.03574864 40.90066599 28.36821016 31.61908141\n",
            " 34.64751088 31.23312751 28.45978783 20.66295316 27.92996503 36.42013462\n",
            " 26.73295293 27.02732915 20.50594344 35.98188949 25.67330733 21.84032205\n",
            " 34.83066622 27.41978545 32.28627071 28.18505482 40.68486263 26.13119568\n",
            " 30.51364697 24.12312536 32.06382894 34.32048664 30.51364697 24.52208406\n",
            " 24.698737   27.56365435 39.14118307 43.45820232 26.56944081 33.48978521\n",
            " 31.4882173  27.6355888  17.57559403 34.6671541  39.7167947  31.2069819\n",
            " 21.46750897 21.81417643 34.1373313  23.73717147 32.71137502 40.20082867\n",
            " 17.73260375 15.61331256 27.39363983 38.95802773 35.47821231 19.05384154\n",
            " 38.22540637 37.76101563 20.27049686 29.33627809 35.77258853 35.44556429\n",
            " 31.06961539 18.66138525 35.03346478 35.58943319 27.4982223  40.06346216\n",
            " 27.72066407 34.2746978  39.02996218 20.70874199 34.85681184 28.21120044\n",
            " 25.14348453 31.67137264 31.94610565 25.50979521 20.15927598 34.11118568\n",
            " 18.5894508  36.21083366 28.5513655  37.11997194 26.4320743  15.37786598\n",
            " 25.74524178 24.12962776 31.57979497 29.05504268 28.36821016 35.26891135\n",
            " 26.52365197 35.07925361 32.84874153 36.00803511 29.26434364 32.57400852\n",
            " 34.04575363 31.75644791 31.57979497 28.67559118 27.95611065 21.40207692\n",
            " 35.89031182 25.46400637 32.29277311 31.06961539 27.16469566 35.36048902\n",
            " 20.64330994 33.48978521 23.92032681 34.64751088 34.71294293 26.54329519\n",
            " 26.98804272 38.28433603 38.77487239 24.50907926 30.58558143 31.93960325\n",
            " 24.3127831  23.71102585 28.32242132 18.96226387 26.64137526 23.42979044\n",
            " 24.63330495 30.49400376 30.33049164 33.11697214 32.26662749 31.25277073\n",
            " 35.29505697 25.67330733 22.23277834 17.20278095 35.31470019 17.22892657\n",
            " 29.43435816 17.85032704 35.1969769  25.58172966 34.36627547 27.88417619\n",
            " 30.52014937 34.43820993 18.90997264 28.99611303 33.35241871 36.35470257\n",
            " 32.24698428 25.62751849 24.52208406 24.42400399 25.52293602 13.84068882\n",
            " 41.86223152 27.53750873 32.33205955 22.5794458  20.20506481 36.72101325\n",
            " 29.38856932 18.84454059 31.44242847 34.94188711 35.31470019 30.16047711\n",
            " 39.97838689 36.71451085 34.41206431 31.61908141 29.63715671 22.67102347\n",
            " 39.1869719  25.13698213 29.45400138 27.17119806 31.78259353 34.99417834\n",
            " 29.17926837 21.29085603 28.36821016 35.36048902 42.97416835 21.84682444\n",
            " 34.18312014 33.67294055 27.72716647 39.12153985 29.17926837 34.40556191\n",
            " 19.34821777 27.74680969 35.15769046 31.59943819 30.55943581 24.3062807\n",
            " 24.3062807  25.71909616 29.06154508 33.28698666 31.54714696 26.73295293\n",
            " 32.50207407 23.01769093 34.11118568 31.85452798 24.8361035  32.91417358\n",
            " 19.58366434 29.40821254 25.20891658 43.82451299 28.62980235 33.58786528\n",
            " 24.90803795 25.34628308 36.32855695 39.0496054  43.73293532 32.4104964\n",
            " 26.93575148 32.5413605  35.03996718 40.01767333 26.8245306  26.45171752\n",
            " 28.62329995 25.30049425 25.39207192 32.98610803 33.83645267 40.01767333\n",
            " 30.07540184 42.67979212 24.7052394  33.81680946 35.15118806 18.13156245\n",
            " 29.24470042 26.79838498 29.54557905 17.16349452 28.78030967 24.40436077\n",
            " 27.19084127 33.44399638 39.85416121 35.0596104  32.05732654 30.6706567\n",
            " 18.21663772 34.8045206  34.11118568 37.01538947 41.56785529 43.68714649\n",
            " 31.41628285 34.30084342 35.28855457 29.17926837 23.22699189 28.74102324\n",
            " 40.15503983 23.42979044 41.63978974 27.92996503 30.10154746 29.17276597\n",
            " 28.71487762 36.67522441 35.17083128 31.85452798 39.86066361 26.20313013\n",
            " 37.4536346  35.66136765 20.10698474 38.89259568 29.87260329 31.96574887\n",
            " 29.78102562 25.34628308 41.07731893 30.2847028  32.86838475 19.90418619\n",
            " 21.39557452 39.53363936 28.51207906 25.30049425 27.75331209 38.72258116\n",
            " 17.09156007 38.03574864 33.83645267 25.25470542 25.9022515  32.33856195\n",
            " 26.75259614 32.91417358 40.99224366 32.88802796 23.22699189 25.32013747\n",
            " 40.29240634 32.20119544 20.87225412 41.35855434 21.3824337  29.6110111\n",
            " 43.82451299 14.96576647 42.92837952 18.0531256  23.85489476 15.08999216\n",
            " 25.73873938 23.1092686  26.13119568 30.00346739 23.87453797 18.03348238\n",
            " 14.52752134 39.92609566 14.04348738 24.03154769 31.06311299 24.37821516\n",
            " 32.22083866 42.94802273 26.47786314 12.17278357 41.35855434 33.67294055\n",
            " 23.59330256 29.63715671 34.96803273 33.03189687 33.02539447 29.31663487\n",
            " 35.31470019 41.22118783 25.55558404 35.1969769  36.51171229 23.20084627\n",
            " 31.42942367 26.13119568 27.85803058 30.76873677 29.12697713 26.45821992\n",
            " 18.45208429 31.24626833 33.76451822 36.69486763 27.85803058 18.17735128\n",
            " 28.64294317 26.41243108 29.91839212 33.92803034 35.24276573 41.07731893\n",
            " 27.60944319 43.68714649 38.24504959 32.89453036 30.821028   36.35470257\n",
            " 38.00960302 26.45171752 33.28698666 23.7764579  37.8525933  31.59943819\n",
            " 29.68294555 28.66908879 36.72101325 32.96646481 33.01225365 34.29434102\n",
            " 20.22470803 27.46557428 27.55715195 27.90381941 29.80066884 27.81224174\n",
            " 21.61137787  5.9719197  24.37821516 37.61714672 33.9999648  30.23891397\n",
            " 29.87260329 20.63680754 29.68294555 29.48014699 35.36048902 31.94610565\n",
            " 22.90647004 35.51749874 24.77067145 35.56978998 31.01732416 32.57400852\n",
            " 19.46594106 11.27665009 35.56978998 24.19505982 28.48593345 34.30084342\n",
            " 41.22118783 27.44593106 35.10539923 40.2531199  25.57522726 23.52787051\n",
            " 43.82451299 29.17926837 21.33014246 29.87260329 36.17154723 22.27856718\n",
            " 34.71294293 22.00383417 21.49365458 24.67909378 31.18733868 20.64330994\n",
            " 29.93803534 34.98767594 32.56750612 17.22242417 31.30506196 39.99803011\n",
            " 33.09732892 26.68716409 25.18277096 22.9719021  41.75101063 26.58908402\n",
            " 29.04190186 39.41591608 22.87382203 23.08312298 33.12347454 35.01382156\n",
            " 29.48014699 20.75453083 43.82451299 20.06119591 21.44786575 29.45400138\n",
            " 38.33662726 31.69751826 36.11925599 33.88224151 24.95382679 41.61364413\n",
            " 30.09504506 26.98154032 27.83838736 33.6075085  21.9122565  19.37436339\n",
            " 28.45978783 22.00383417 35.75294532 39.69715149 13.99769854 28.99611303\n",
            " 39.07575102 39.1869719  27.64872962 28.04768832 30.23891397 34.2746978\n",
            " 25.2350622  29.43435816 28.59715433 37.02189187 29.6175135  27.83838736\n",
            " 29.84645767 15.44980044 33.42435316 26.91610827 23.45593606 34.83066622\n",
            " 27.03383155 34.43820993 36.28276812 28.89803296 25.50979521 31.99189449\n",
            " 41.46977522 38.00960302 33.90188473 26.10505006 26.8245306  24.73138502\n",
            " 27.54401113 23.17470065 36.11925599 24.95382679 28.57751112 33.21505221\n",
            " 20.94418857 37.47978022 29.34278049 37.11346954 27.58329757 32.59365174\n",
            " 17.06541445 27.70102086 29.75488    27.58329757 32.403994   28.07383393\n",
            " 33.6075085  28.1589092  36.46592345 23.92032681 38.58521465 41.77065385\n",
            " 20.80682206 16.37207953 37.22469043 39.23926314 34.20926575 22.67102347\n",
            " 33.81030706 35.75294532 30.84067122 22.14770307 26.4320743  27.95611065\n",
            " 29.80066884 16.94769116 29.27084604 22.94575648 34.50364198 33.05154008\n",
            " 27.39363983 29.77452322 22.71681231 34.50364198 27.26927813 26.22277334\n",
            " 37.66293556 32.75066146 17.39243869 41.19504221 24.91454035 30.12119068\n",
            " 36.28276812 35.91645744 29.38856932 32.88802796 37.11346954 28.76066645\n",
            " 29.98382418 32.45628523 23.65873462 33.11697214 26.2947078  22.02997978\n",
            " 27.6355888  33.81030706 19.62945318 24.19505982 20.68259638 38.00960302\n",
            " 37.29662488 25.60137288 31.18733868 26.22277334 26.11155246 31.04346977\n",
            " 34.5297876  34.01960801 33.42435316 28.92417858 29.89224651 28.57751112\n",
            " 30.53979259 30.76873677 24.60715933 36.04732154 27.53750873 35.08575601\n",
            " 31.43592607 19.13891681 27.6290864  25.34628308 35.10539923 34.32048664\n",
            " 30.93224889 37.55171467 33.71872939 20.13313036 31.16119306 27.86453298\n",
            " 29.87260329 36.4004914  28.53172228 24.51558166 28.04768832 37.17890159\n",
            " 23.1092686  28.58401352 34.70644054 28.04768832 25.14348453 31.64522703\n",
            " 32.58714934 38.9122389  36.32855695 28.92417858]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"student.csv\")\n",
        "\n",
        "# Select features and target\n",
        "X = df[[\"Math\", \"Reading\"]].values\n",
        "Y = df[\"Writing\"].values\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training Feature Shape:\", X_train.shape)\n",
        "print(\"Testing Feature Shape:\", X_test.shape)\n",
        "print(\"Training Label Shape:\", Y_train.shape)\n",
        "print(\"Testing Label Shape:\", Y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZImrsm21mHzc",
        "outputId": "e94a462f-3401-425e-8919-d291fb1efbf2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Feature Shape: (800, 2)\n",
            "Testing Feature Shape: (200, 2)\n",
            "Training Label Shape: (800,)\n",
            "Testing Label Shape: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 and 5\n",
        "\n",
        "# Cost Function (Mean Squared Error)\n",
        "def cost_function(X, Y, W):\n",
        "    n = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)\n",
        "    return cost\n",
        "\n",
        "\n",
        "#  Test Case\n",
        "\n",
        "X_test = np.array([[1, 2],\n",
        "                   [3, 4],\n",
        "                   [5, 6]])\n",
        "\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong\")\n",
        "\n",
        "print(\"Cost Function Output:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaLSXBdemZSS",
        "outputId": "660b9623-fcd3-4538-e322-ad5fc61210c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost Function Output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 and 7\n",
        "\n",
        "\n",
        "# Cost Function (Mean Squared Error)\n",
        "def cost_function(X, Y, W):\n",
        "    n = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)\n",
        "    return cost\n",
        "\n",
        "# Gradient Descent Function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    m = len(Y)\n",
        "    cost_history = [0] * iterations\n",
        "    W_update = W.copy()\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Step 1: Hypothesis\n",
        "        Y_pred = np.dot(X, W_update)\n",
        "        # Step 2: Loss\n",
        "        loss = Y_pred - Y\n",
        "        # Step 3: Gradient\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "        # Step 4: Update Weights\n",
        "        W_update = W_update - alpha * dw\n",
        "        # Step 5: Compute Cost\n",
        "        cost_history[i] = cost_function(X, Y, W_update)\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "#Test Case\n",
        "np.random.seed(0)  # Reproducibility\n",
        "X_test = np.random.rand(100, 3)  # 100 samples, 3 features\n",
        "Y_test = np.random.rand(100)\n",
        "W_init = np.random.rand(3)       # Initial weights\n",
        "\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "final_params, cost_history = gradient_descent(X_test, Y_test, W_init, alpha, iterations)\n",
        "\n",
        "# Output results\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"First 10 Cost Values:\", cost_history[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI4aThQOnmMR",
        "outputId": "bd54d345-cc3c-40ad-fdc1-4d76fc85c98b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "First 10 Cost Values: [np.float64(0.10711197094660153), np.float64(0.10634880599939901), np.float64(0.10559826315680618), np.float64(0.10486012948320558), np.float64(0.1041341956428534), np.float64(0.10342025583900626), np.float64(0.1027181077540776), np.float64(0.1020275524908062), np.float64(0.10134839451441931), np.float64(0.1006804415957737)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 8 and 9\n",
        "\n",
        "\n",
        "# Root Mean Squared Error (RMSE)\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Root Mean Squared Error\n",
        "    \"\"\"\n",
        "    rmse_val = np.sqrt(np.mean((Y_pred - Y) ** 2))\n",
        "    return rmse_val\n",
        "\n",
        "# R-Squared (Coefficient of Determination)\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculate R-Squared Error\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)       # Total Sum of Squares\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)       # Residual Sum of Squares\n",
        "    r2_val = 1 - (ss_res / ss_tot)\n",
        "    return r2_val\n",
        "\n",
        "# Test Example\n",
        "np.random.seed(0)\n",
        "X_test = np.random.rand(100, 2)\n",
        "Y_test = np.random.rand(100)\n",
        "W_test = np.random.rand(2)\n",
        "\n",
        "# Predicted values using simple dot product\n",
        "Y_pred_test = np.dot(X_test, W_test)\n",
        "\n",
        "# Evaluate\n",
        "rmse_val = rmse(Y_test, Y_pred_test)\n",
        "r2_val = r2(Y_test, Y_pred_test)\n",
        "\n",
        "print(\"RMSE:\", rmse_val)\n",
        "print(\"R-Squared:\", r2_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjJVYvR2obXb",
        "outputId": "6277b846-d4fc-4184-a8b2-e13b45fba5c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.5433575409410246\n",
            "R-Squared: -2.2439650615927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 and 11\n",
        "\n",
        "# ---------------- Cost Function ----------------\n",
        "def cost_function(X, Y, W):\n",
        "    n = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)\n",
        "    return cost\n",
        "\n",
        "# ---------------- Gradient Descent ----------------\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    m = len(Y)\n",
        "    cost_history = [0] * iterations\n",
        "    W_update = W.copy()\n",
        "\n",
        "    for i in range(iterations):\n",
        "        Y_pred = np.dot(X, W_update)\n",
        "        loss = Y_pred - Y\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "        W_update = W_update - alpha * dw\n",
        "        cost_history[i] = cost_function(X, Y, W_update)\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# ---------------- RMSE ----------------\n",
        "def rmse(Y, Y_pred):\n",
        "    return np.sqrt(np.mean((Y_pred - Y) ** 2))\n",
        "\n",
        "# ---------------- R-Squared ----------------\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "# ---------------- Main Function ----------------\n",
        "def main():\n",
        "    # Step 1: Load dataset\n",
        "    data = pd.read_csv(\"student.csv\")\n",
        "\n",
        "    # Step 2: Prepare feature matrix and target vector\n",
        "    X = data[[\"Math\", \"Reading\"]].values\n",
        "    Y = data[\"Writing\"].values\n",
        "\n",
        "    # Step 3: Split into training and test sets (80%-20%)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights, learning rate, iterations\n",
        "    W_init = np.zeros(X_train.shape[1])\n",
        "    alpha = 0.00001\n",
        "    iterations = 1000\n",
        "\n",
        "    # Step 5: Perform gradient descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W_init, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate model\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "    # Step 9: Findings\n",
        "    print(\"\\n--- Findings ---\")\n",
        "    print(\"1. Model Performance: Acceptable if RMSE is low and R² close to 1.\")\n",
        "    print(\"2. Experiment with learning rate (alpha):\")\n",
        "    print(\"   - Increase alpha -> faster convergence but risk overshooting.\")\n",
        "    print(\"   - Decrease alpha -> slower convergence but safer.\")\n",
        "    print(\"3. Overfitting / Underfitting check: Compare train/test errors.\")\n",
        "\n",
        "# Execute main\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv7-LlAZowp2",
        "outputId": "e4300331-5333-4273-bcb6-cf9938ab5698"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [np.float64(2013.165570783755), np.float64(1640.286832599692), np.float64(1337.0619994901588), np.float64(1090.4794892850578), np.float64(889.9583270083234), np.float64(726.8940993009545), np.float64(594.2897260808594), np.float64(486.4552052951635), np.float64(398.7634463599484), np.float64(327.4517147324688)]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n",
            "\n",
            "--- Findings ---\n",
            "1. Model Performance: Acceptable if RMSE is low and R² close to 1.\n",
            "2. Experiment with learning rate (alpha):\n",
            "   - Increase alpha -> faster convergence but risk overshooting.\n",
            "   - Decrease alpha -> slower convergence but safer.\n",
            "3. Overfitting / Underfitting check: Compare train/test errors.\n"
          ]
        }
      ]
    }
  ]
}